{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imagerecognition.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMm70m3hYDy+XWo+HOqjZhL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrien50/humanactivityrecognition/blob/main/imagerecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdTBcDMkAiRM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEVcSu1YAivr"
      },
      "source": [
        "Image Recognition (Classification)\r\n",
        "\r\n",
        "Image recognition refers to the task of inputting an image into a neural network and having it output some kind of label for that image. The label that the network outputs will correspond to a pre-defined class. There can be multiple classes that the image can be labeled as, or just one. If there is a single class, the term \"recognition\" is often applied, whereas a multi-class recognition task is often called \"classification\".\r\n",
        "\r\n",
        "A subset of image classification is object detection, where specific instances of objects are identified as belonging to a certain class like animals, cars, or people.\r\n",
        "\r\n",
        "\r\n",
        "Feature Extraction\r\n",
        "\r\n",
        "In order to carry out image recognition/classification, the neural network must carry out feature extraction. Features are the elements of the data that you care about which will be fed through the network. In the specific case of image recognition, the features are the groups of pixels, like edges and points, of an object that the network will analyze for patterns.\r\n",
        "\r\n",
        "Feature recognition (or feature extraction) is the process of pulling the relevant features out from an input image so that these features can be analyzed. Many images contain annotations or metadata about the image that helps the network find the relevant features.\r\n",
        "reference https://stackabuse.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNhjjw8CEYlW"
      },
      "source": [
        "The first thing we should do is import the necessary libraries. I'll show how these imports are used as we go, but for now know that we'll be making use of Numpy, and various modules associated with Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17QP0Yp5ArBc"
      },
      "source": [
        "import numpy\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\r\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\r\n",
        "from keras.constraints import maxnorm\r\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B4ufeP8Em6x"
      },
      "source": [
        "# Set random seed for purposes of reproducibility\r\n",
        "seed = 21"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpdTcHowE1WV"
      },
      "source": [
        "from keras.datasets import cifar10"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F7TItxxE6Bu",
        "outputId": "64d173c4-2574-41e1-b3e3-ac35538fd265"
      },
      "source": [
        "# loading in the data\r\n",
        "     \r\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lez51mcnFHSI"
      },
      "source": [
        "In most cases you will need to do some preprocessing of your data to get it ready for use, but since we are using a prepackaged dataset, very little preprocessing needs to be done. One thing we want to do is normalize the input data.\r\n",
        "\r\n",
        "If the values of the input data are in too wide a range it can negatively impact how the network performs. In this case, the input values are the pixels in the image, which have a value between 0 to 255.\r\n",
        "\r\n",
        "So in order to normalize the data we can simply divide the image values by 255. To do this we first need to make the data a float type, since they are currently integers. We can do this by using the astype() Numpy command and then declaring what data type we want:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mIRUUekFOhF"
      },
      "source": [
        "# normalize the inputs from 0-255 to between 0 and 1 by dividing by 255\r\n",
        "    \r\n",
        "X_train = X_train.astype('float32')\r\n",
        "X_test = X_test.astype('float32')\r\n",
        "X_train = X_train / 255.0\r\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ5emJFiFoRH"
      },
      "source": [
        "Another thing we'll need to do to get the data ready for the network is to one-hot encode the values. I won't go into the specifics of one-hot encoding here, but for now know that the images can't be used by the network as they are, they need to be encoded first and one-hot encoding is best used when doing binary classification.\r\n",
        "\r\n",
        "We are effectively doing binary classification here because an image either belongs to one class or it doesn't, it can't fall somewhere in-between. The Numpy command to_categorical() is used to one-hot encode. This is why we imported the np_utils function from Keras, as it contains to_categorical().\r\n",
        "\r\n",
        "We also need to specify the number of classes that are in the dataset, so we know how many neurons to compress the final layer down to:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YeJLDMAFngI"
      },
      "source": [
        "# one hot encode outputs\r\n",
        "y_train = np_utils.to_categorical(y_train)\r\n",
        "y_test = np_utils.to_categorical(y_test)\r\n",
        "class_num = y_test.shape[1]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itbENtAbGrAK"
      },
      "source": [
        "Create the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1-3WoJQGp1Q"
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvWcxST3G6uP"
      },
      "source": [
        "The first layer of our model is a convolutional layer. It will take in the inputs and run convolutional filters on them.\r\n",
        "\r\n",
        "When implementing these in Keras, we have to specify the number of channels/filters we want (that's the 32 below), the size of the filter we want (3 x 3 in this case), the input shape (when creating the first layer) and the activation and padding we need.\r\n",
        "\r\n",
        "As mentioned, relu is the most common activation, and padding='same' just means we aren't changing the size of the image at all:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztzdvvKpG7ue"
      },
      "source": [
        "model.add(Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))\r\n",
        "model.add(Activation('relu'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiEcIcHDHHUY"
      },
      "source": [
        "ou can also string the activations and poolings together, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtheMmHbHOTo"
      },
      "source": [
        "model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgarQzIFHVPi"
      },
      "source": [
        "Now we will make a dropout layer to prevent overfitting, which functions by randomly eliminating some of the connections between the layers (0.2 means it drops 20% of the existing connections):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-gWXd9JHPL8"
      },
      "source": [
        "model.add(Dropout(0.2))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKKOn23iHiIO"
      },
      "source": [
        "We may also want to do batch normalization here. Batch Normalization normalizes the inputs heading into the next layer, ensuring that the network always creates activations with the same distribution that we desire:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZNtmQtEHjKI"
      },
      "source": [
        "model.add(BatchNormalization())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95TPGXGGIHtc"
      },
      "source": [
        "Now comes another convolutional layer, but the filter size increases so the network can learn more complex representations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfMfhLYlIItM"
      },
      "source": [
        "model.add(Conv2D(64, (3, 3), padding='same'))\r\n",
        "model.add(Activation('relu'))\r\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgBzauFaIWv6"
      },
      "source": [
        "Here's the pooling layer, as discussed before this helps make the image classifier more robust so it can learn relevant patterns. There's also the dropout and batch normalization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMD6FQb2IX2Q"
      },
      "source": [
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(BatchNormalization())"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8NlW9i_IvOY"
      },
      "source": [
        "You can vary the exact number of convolutional layers you have to your liking, though each one adds more computation expenses. Notice that as you add convolutional layers you typically increase their number of filters so the model can learn more complex representations. If the numbers chosen for these layers seems somewhat arbitrary, just know that in general, you increase filters as you go on and it's advised to make them powers of 2 which can grant a slight benefit when training on a GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPufVPAdJGAy"
      },
      "source": [
        "It's important not to have too many pooling layers, as each pooling discards some data. Pooling too often will lead to there being almost nothing for the densely connected layers to learn about when the data reaches them.\r\n",
        "\r\n",
        "The exact number of pooling layers you should use will vary depending on the task you are doing, and it's something you'll get a feel for over time. Since the images are so small here already we won't pool more than twice.\r\n",
        "\r\n",
        "You can now repeat these layers to give your network more representations to work off of:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7arOjttgJOHW"
      },
      "source": [
        "model.add(Conv2D(64, (3, 3), padding='same'))\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(BatchNormalization())\r\n",
        "    \r\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(BatchNormalization())"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJfMeNVRJlco"
      },
      "source": [
        "After we are done with the convolutional layers, we need to Flatten the data, which is why we imported the function above. We'll also add a layer of dropout again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgGHrr6CJT1f"
      },
      "source": [
        "model.add(Flatten())\r\n",
        "model.add(Dropout(0.2))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW5w_zQcJ3E5"
      },
      "source": [
        "Now we make use of the Dense import and create the first densely connected layer. We need to specify the number of neurons in the dense layer. Note that the numbers of neurons in succeeding layers decreases, eventually approaching the same number of neurons as there are classes in the dataset (in this case 10). The kernel constraint can regularize the data as it learns, another thing that helps prevent overfitting. This is why we imported maxnorm earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-5YEvthIwh-"
      },
      "source": [
        "model.add(Dense(256, kernel_constraint=maxnorm(3)))\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(BatchNormalization())\r\n",
        "    \r\n",
        "model.add(Dense(128, kernel_constraint=maxnorm(3)))\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(BatchNormalization())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac8vay5sKK_z"
      },
      "source": [
        "In this final layer, we pass in the number of classes for the number of neurons. Each neuron represents a class, and the output of this layer will be a 10 neuron vector with each neuron storing some probability that the image in question belongs to the class it represents.\r\n",
        "\r\n",
        "Finally, the softmax activation function selects the neuron with the highest probability as its output, voting that the image belongs to that class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNXeIvsjKMPv"
      },
      "source": [
        "model.add(Dense(class_num))\r\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Cs89TdJLBEy"
      },
      "source": [
        "Now that we've designed the model we want to use, we just have to compile it. Let's specify the number of epochs we want to train for, as well as the optimizer we want to use.\r\n",
        "\r\n",
        "The optimizer is what will tune the weights in your network to approach the point of lowest loss. The Adam algorithm is one of the most commonly used optimizers because it gives great performance on most problems:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la9aYoqgLB-2"
      },
      "source": [
        "epochs = 25\r\n",
        "optimizer = 'adam'"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lxjzlJiLIgn"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajSw6beULNtb"
      },
      "source": [
        "We can print out the model summary to see what the whole model looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3PYs9_YLOZe",
        "outputId": "8967a833-2434-42ba-def7-0401e07d30fc"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               2097408   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 2,273,944\n",
            "Trainable params: 2,272,536\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzZG86L1LrCC"
      },
      "source": [
        "Now we get to training the model. To do this, all we have to do is call the fit() function on the model and pass in the chosen parameters.\r\n",
        "\r\n",
        "Here's where I use the seed I chose, for the purposes of reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H91uTWrXLr6C",
        "outputId": "b48fb7ae-4cf3-40f7-d23d-7270b502ddfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "numpy.random.seed(seed)\r\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "782/782 [==============================] - 472s 602ms/step - loss: 2.0987 - accuracy: 0.2682 - val_loss: 1.7905 - val_accuracy: 0.4193\n",
            "Epoch 2/25\n",
            "782/782 [==============================] - 468s 599ms/step - loss: 1.7316 - accuracy: 0.4338 - val_loss: 1.4961 - val_accuracy: 0.5085\n",
            "Epoch 3/25\n",
            "782/782 [==============================] - 466s 596ms/step - loss: 1.4562 - accuracy: 0.5180 - val_loss: 1.3253 - val_accuracy: 0.5453\n",
            "Epoch 4/25\n",
            "782/782 [==============================] - 464s 593ms/step - loss: 1.2725 - accuracy: 0.5605 - val_loss: 1.1260 - val_accuracy: 0.6287\n",
            "Epoch 5/25\n",
            "782/782 [==============================] - 463s 593ms/step - loss: 1.1167 - accuracy: 0.6278 - val_loss: 1.1239 - val_accuracy: 0.6279\n",
            "Epoch 6/25\n",
            "782/782 [==============================] - 460s 588ms/step - loss: 1.0028 - accuracy: 0.6586 - val_loss: 0.9267 - val_accuracy: 0.6956\n",
            "Epoch 7/25\n",
            "782/782 [==============================] - 459s 587ms/step - loss: 0.9243 - accuracy: 0.6866 - val_loss: 0.9757 - val_accuracy: 0.6708\n",
            "Epoch 8/25\n",
            "782/782 [==============================] - 458s 586ms/step - loss: 0.8666 - accuracy: 0.7121 - val_loss: 0.9660 - val_accuracy: 0.6782\n",
            "Epoch 9/25\n",
            "782/782 [==============================] - 464s 593ms/step - loss: 0.7974 - accuracy: 0.7372 - val_loss: 0.7952 - val_accuracy: 0.7386\n",
            "Epoch 10/25\n",
            "782/782 [==============================] - 468s 599ms/step - loss: 0.7700 - accuracy: 0.7404 - val_loss: 0.9445 - val_accuracy: 0.6906\n",
            "Epoch 11/25\n",
            "782/782 [==============================] - 458s 586ms/step - loss: 0.7257 - accuracy: 0.7562 - val_loss: 0.7442 - val_accuracy: 0.7544\n",
            "Epoch 12/25\n",
            "782/782 [==============================] - 462s 590ms/step - loss: 0.6943 - accuracy: 0.7666 - val_loss: 0.7323 - val_accuracy: 0.7679\n",
            "Epoch 13/25\n",
            "782/782 [==============================] - 473s 604ms/step - loss: 0.6689 - accuracy: 0.7778 - val_loss: 0.7240 - val_accuracy: 0.7645\n",
            "Epoch 14/25\n",
            "782/782 [==============================] - 472s 603ms/step - loss: 0.6409 - accuracy: 0.7852 - val_loss: 0.7196 - val_accuracy: 0.7741\n",
            "Epoch 15/25\n",
            "782/782 [==============================] - 467s 597ms/step - loss: 0.6219 - accuracy: 0.7954 - val_loss: 0.7408 - val_accuracy: 0.7593\n",
            "Epoch 16/25\n",
            "776/782 [============================>.] - ETA: 3s - loss: 0.6094 - accuracy: 0.8031"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea5xrWwQL4Vm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DJgkS6HL4bq"
      },
      "source": [
        "Now we can evaluate the model and see how it performed. Just call model.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DEDnVRdL-7J"
      },
      "source": [
        "# Model evaluation\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHRcwjVVL_dW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}